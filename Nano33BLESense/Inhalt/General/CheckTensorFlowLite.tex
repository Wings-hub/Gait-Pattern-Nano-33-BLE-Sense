%%%%%%%%%%%%%%%%%%%%
%
% $Autor: Wings $
% $Datum: 2020-02-24 14:29:03Z $
% $Pfad: komponenten/Bilderkennung/Produktspezifikation/CorelTPU/Ausarbeitung/Kapitel/Software.tex $
% $Version: 1791 $
%
%
%%%%%%%%%%%%%%%%%%%


\chapter{TensorFlow Lite}

\section{Was ist TensorFlow Lite?}
Oft sollen die trainierten Modelle nicht auf dem PC verwendet werden, auf dem sie trainiert wurden sondern auf mobilen Geräten wie Mobiltelefonen
oder Mikrocontrollern oder anderen eingebetteten Systemen. Dort stehen allerdings begrenzte Rechen- und Speicherkapazitäten zur Verfügung.
TensorFlow Lite ist ein ganzes Framework, dass die Konvertierung von Modellen in ein spezielles Format und deren Optimierung bezüglich Größe
 und Geschwindigkeit erlaubt, sodass die Modelle mit TensorFlow Lite Interpretern dann auf Mobil-, Embedded- und IoT-Geräten ausgeführt werden 
 können. \cite{Google.09.10.2020}\cite{Warden:2020}
 
 Üblicherweise rechnen neuronale Netze in Gleitkommazahlen mit 32 Bit. Die Gewichte können dabei sehr kleine Werte nahe Null annehmen. 
 Deshalb können die Modelle nicht ohne weiteres auf Systemen ausgeführt werden, die gar nicht mit langen Gleitkommazahlen rechnen können, 
 sondern etwa nur mit 8 Bit Integern. Das ist vor allem bei kleinen KI-Chips der Fall, auf denen auf Grund von Größe und Stromverbrauch nur 
 wenige Transistoren untergebracht werden können. Deshalb muss das neuronale Netz eine Transformation durchlaufen, \glqq bei der aus langen Gleitkommazahlen
mit unterschiedlicher Genauigkeit über den darstellbaren Wertebereich (Gleitkommazahlen bilden den Zahlenbereich um den Nullpunkt viel feiner
ab als sehr große oder kleine Werte) kurze Ganzzahlen mit gleichbleibender Genauigkeit in einem begrenzten Wertebereich werden \grqq \cite{Heise:2020}.
Diese Quantisierung findet bei der Überführung in das TensorFlow Lite Format statt.
 
\section{Ablauf}
 
Der Workflow für den Einsatz von TensorFlowLite besteht aus vier Schritten:
 
\begin{enumerate}
  \item Wählen eines Modells
 
      Es kann ein bestehendes, trainiertes Modell übernommen oder selbst trainiert werden oder ein Modell von Grund auf erstellt und trainiert werden.
  \item Konvertieren des Modells

        Wenn ein benutzerdefiniertes Modell verwendet wird, das dementsprechend nicht im TensorFlow Lite Format vorliegt, muss es mit dem  TensorFlow Lite-Konverter in das Format überführt werden.
  \item Bereitstellung auf dem Gerät
  
        Zur Ausführung des Modells auf dem Gerät steht der TensorFlow Lite-Interpreter mit APIs in vielen Sprachen bereit.

 \item Optimierung des Modells
 
       Es stehen weitere Tools zur Optimierung zur Verfügung, um Modellgröße und Ausführungsgeschwindigkeit zu verbessern.
\end{enumerate}

\section{TensorFlow Lite-Konverter}

Der TensorFlow Lite-Konverter ist ein Tool, das als Python-API verfügbar ist und trainierte TensorFlow-Modelle in das TensorFlow Lite-Format konvertiert. 
Dazu wird das Keras-Modell in die Form eines FlatBuffers gebracht, ein spezielles platzsparend Dateiformat. Im Allgemeinen wird durch das Konvertieren von Modellen 
die Dateigröße reduziert und Optimierungen eingeführt, ohne die Genauigkeit zu beeinträchtigen. Der TensorFlow Lite-Konverter bietet weiterhin Optionen, mit denen 
die Dateigröße weiter reduziert und die Ausführungsgeschwindigkeit erhöht werden kann, 
was mit mit einigen Kompromissen bezüglich der Genauigkeit einher geht. \cite{Google.09.10.2020}\cite{Warden:2020}

Eine Möglichkeit zur Optimierung ist Quantisierung. Während die Gewichte der Modelle üblicherweise als 32-bit Fließkommazahlen gespeichert werden, 
können sie mit einem nur minimalen Verlust an Genauigkeit des Modells in 8-bit Ganzzahlen überführt werden. Das spart Speicherplatz und erlaubt zudem
eine schnellere Berechnung, das sich die CPU mit Ganzzahlen leichter tut. \cite{Warden:2020}

\section{TensorFlow Lite-Interpreter}
Der TensorFlow Lite-Interpreter ist eine Bibliothek, die ein entsprechend konvertiertes TensorFlow-Lite-Modell unter Verwendung der effizientesten
Operationen für das gegebene Gerät ausführt, indem sie die im Modell definierten Operationen auf die Eingabe anwendet und Zugriff auf die
Ausgabe bietet. Er funktioniert plattformübergreifend und bietet eine einfache API zum Ausführen von TensorFlow Lite-Modellen aus Java, Swift, 
Objective-C, C ++ und Python.\cite{Google.09.10.2020}\cite{Warden:2020}

Um die Hardwarebeschleunigung auf verschiedenen Geräten zu nutzen, kann der TensorFlow Lite-Interpreter mit \glqq Delegates\grqq konfiguriert werden. 
Diese nutzen Gerätebeschleuniger wie die GPU und den digitalen Signalprozessor (DSP). \cite{Google.09.10.2020}

\chapter{TensorFlow Lite und Jetson Nano}

\section{Kompatibilität}
Im Vergleich zu anderen KI-Systemen wie der Edge TPU ist der Jetson Nano von NVIDIA flexibler bezüglich der Modelleigenschaften. 
Wie \cite{Stock.07.01.2021} zeigt, können auf dem Jetson Nano auf 16- und 32-bit-Modelle verwendet werden. Daher gibt es viele verschiedene
Modellformate, die mit dem Jetson Nano kompatibel sind. Oft werden Keras' .h5- oder TensorFlows .pb-Modelle zu TensorRT (uff) konvertiert, um sie auf dem
Jetson Nano zu verwenden. Es scheint auf Grund der vielfältigen Möglichkeiten noch nicht all zu viel Literatur zur Verwendung von TensorFlow Lite auf dem
Jetson Nano zu geben. Allerdings ist TensorFlow Lite für die Verwendung auf ARM CPU edge devices optimiert \cite{Jain:2020}. Da auch im Jetson Nano ein 
ARM Cortex-A57 Prozessor verwendet wird, bietet sich die Verwendung von TensorFlow Lite auf dem Jetson Nano jedoch offensichtlich an.

\section{Modell konvertieren}
Empfohlen wird die Konvertierung eines Modells im SavedModel-Format. Nach dem Training sollte deshalb das Modell entsprechend gespeichert werden.
Anschließend können dann die folgenden Schritte erfolgen:

\begin{verbatim}
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
\end{verbatim}

Dies konvertiert das gespeicherte Modell in das tflite-Format und speichert es am vorgegeben Ort mit dem vorgegebenen Namen. \cite{Google.04.11.2020}
'wb' spezifiziert dabei lediglich, dass im binary mode geschrieben wird.

Zusätzlich kann dieses Modell nun wie bereits beschrieben optimiert werden. Dazu muss vor der tatsächlichen Konvertierung die Optimierung spezifiziert werden 
\cite{Warden:2020}:

\begin{verbatim}
converter.optimizations = [tf.lite.Optimize.DEFAULT]
\end{verbatim}
Es gibt neben der DEFAULT-Variante noch die Optionen, gezielt die Größe oder Latenz zu optimieren.
Nach Angaben von \cite{Google.13.01.2021} sind diese jedoch veraltet, sodass DEFAULT verwendet werden sollte.

\section{Modell transferieren}
Eine bequeme Methode, um Modelle und andere größere Dateien zwischen einem PC und dem Jetson Nano zu transferieren ist die Einrichtung
eines Secure SHell FileSystems (SSHFS). Damit kann vom PC aus auf die Ordner des Jetson Nano zugegriffen werden.


Die Installation eines solchen Systems für Windows wird  \url{hier}{https://www.digikey.com/en/maker/projects/getting-started-with-the-nvidia-jetson-nano-part-1-setup/2f497bb88c6f4688b9774a81b80b8ec2}
erklärt. 

Zunächst muss die neueste Version von winfsp heruntergeladen werden:

\url{https://github.com/billziss-gh/winfsp/releases}
Anschließend wird SSHFS-Win installiert: 
\url{https://github.com/billziss-gh/sshfs-win/releases}
Danach kann man in Windows den Explorer/ Dateimanager öffnen. Nach einem Rechtsklick auf 'Dieser PC' wählt man 'Netzwerkadresse hinzufügen'.
Als Netzwerkadresse muss folgendes eingegeben werden:

\SHELL{\textbackslash \textbackslash sshfs \textbackslash<username>@<IP address>}
Wobei für <username> der auf dem Jetson nano genutzte Benutzername und für <IP address> die IP-Adresse angegeben werden muss.

Die IP-Adresse kann man sich vom Jetson Nano mit dem Befehl
\SHELL{\$ ip addr show}
anzeigen lassen.

\section{Ausführen des Modells auf Jetson Nano}
Das TensorFlow Lite Modell kann in verschiedenen Umgebungen verwendet werden. Auf dem Jetson Nano bietet sich die Verwendung der Python-Umgebung an.
Um den TensorFlow Lite Interpreter verwenden zu können, muss TensorFlow auf dem Jetson Nano installiert sein.
Die Installation von TensorFlow auf dem Jetson Nano wird im Kapitel \ref{Kapitel_TF} beschrieben.

Zu Beginn eines Programms, dass ein tflite-Modell auswerten soll, muss TensorFlow importiert werden. Anschließend sollte das Modell durch Angabe des
Speicherpfads geladen und die Tensoren alloziiert werden:

\begin{lstlisting}[frame=none, numbers=none]
import tensorflow as tf

interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()
\end{lstlisting}

Dann kann die Form/ Größe der Input- und Output-Tensoren bestimmt werden, sodass anschließend die Daten entsprechend angepasst werden können.
\begin{lstlisting}[frame=none, numbers=none]
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
\end{lstlisting}

Mit dieser Information können die Eingabedaten, in diesem Beispiel von \ref{Google.24.11.2020} ein zufälliges Array (Denken Sie an den Import von Numpy wenn
Sie dies verwenden wollen), angepasst und als Eingabedaten festgelegt werden:
\begin{lstlisting}[frame=none, numbers=none]
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
\end{lstlisting}

Dann wird der Interpreter ausgeführt und ermittelt das Ergebnis des Modells
\begin{lstlisting}[frame=none, numbers=none]
interpreter.invoke()
\end{lstlisting}

Schließlich kann die Ausgabe des Modells ermittelt und ausgelesen werden:
\begin{lstlisting}[frame=none, numbers=none]
output_data = interpreter.get_tensor(output_details[0]['index'])
\end{lstlisting}

\input{../Inhalt/General/BeispielTFLiteFlaschen}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{TensorFlowLite und Edge TPU}
Es handelt sich um eine light Version von TensorFlow, die für mobile und eingebettete Geräte entwickelt wurde. 
Es erreicht Inferenz mit niedriger Latenz in einer kleinen Binärgröße - sowohl die TensorFlow Lite Modelle als auch 
die Interpreterkerne sind viel kleiner. Sie können ein Modell nicht direkt mit TensorFlow Lite trainieren, sondern müssen Ihr
 Modell mit dem TensorFlow Lite Konverter von einer TensorFlow-Datei (z.B. einer \PYTHON{.pb}-Datei) in eine TensorFlow Lite-Datei 
(eine \PYTHON{.tflite}-Datei) konvertieren.\cite{GoogleCoral:2019}

\bigskip

Der Tensorflowlite besteht aus vier Arbeitsgängen:

\begin{itemize}
	\item Wählen Sie ein Modell (Wählen Sie ein neues Modell 
	oder schulen Sie ein bestehendes Modell.)
	\item Konvertieren (Konvertieren eines TensorFlow-Modells 
	in einen komprimierten flachen Puffer mit dem TensorFlow Lite Converter.)
	\item Bereitstellen Nehmen Sie die komprimierte 
	\PYTHON{.tflite}-Datei und laden Sie sie 
	auf ein mobiles oder eingebettetes Gerät.)
	\item Optimierung ((Quantisieren durch Konvertieren von 
	32-Bit-Floats in effizientere 8-Bit 
	Ganzzahlen).\cite{GoogleTensorFlowLite:2019}
\end{itemize}

\bigskip

\begin{enumerate}
	\item Mit dem TensorFlow Lite Konverter werden 
	TensorFlow-Modelle in ein optimiertes FlatBuffer-Format 
	umgewandelt, so dass sie vom TensorFlow Lite-Interpreter 
	verwendet werden können.\cite{GoogleTensorFlowLite:2019}
	\item TensorFlow Lite Modelle können durch Quantisierung, die 
	32-Bit-Parameterdaten in 8-Bit-Darstellungen umwandelt
	(was für die Edge TPU erforderlich ist), noch kleiner 
	und effizienter gestaltet werden.\cite{GoogleCoral:2019}
\end{enumerate}


\section{Erstellung eines Modells  auf der Edge TPU}
Damit das Edge TPU eine schnelle neuronale Netzwerkleistung bei niedrigen Energiekosten bietet, unterstützt das Edge TPU einen bestimmten Satz von neuronalen Netzwerkoperationen und -architekturen. Dieser Abschnitt beschreibt, welche Arten von Modellen mit dem Edge TPU kompatibel sind und wie Sie sie erstellen können, entweder durch Zusammenstellen eines eigenen TensorFlow-Modells oder durch Umschulung eines bestehenden Modells mit Transfer-Learning.

\subsection{Kompatibilitätsübersicht}

Das Edge TPU ist in der Lage, neuronale Netze mit tiefer Vorwärtskopplung wie z.B. Faltungsneuronale Netze (CNN) auszuführen. Es unterstützt nur TensorFlow Lite Modelle, die vollständig 8-Bit quantisiert und dann speziell für das Edge TPU kompiliert wurden.
TensorFlow unterstützt eine Modelloptimierungstechnik namens Quantisierung, die für das Edge TPU erforderlich ist. Die Quantisierung Ihres Modells bedeutet, dass alle 32-Bit-Fließkommazahlen (wie Gewichte und Aktivierungsausgaben) in die nächsten 8-Bit-Festkommazahlen umgewandelt werden. Dies macht das Modell kleiner und schneller. Und obwohl diese 8-Bit-Darstellungen weniger präzise sein können, wird die Inferenzgenauigkeit des neuronalen Netzwerks nicht wesentlich beeinträchtigt.

\bigskip

Abbildung~\ref{Software:Flowchart} veranschaulicht den grundlegenden Prozess zur Erstellung eines Modells, das mit dem Edge TPU kompatibel ist. Der größte Teil des Workflows verwendet Standard-TensorFlow-Tools. Sobald Sie ein TensorFlow Lite Modell haben, verwenden Sie den Edge TPU Compiler, um eine \PYTHON{.tflite}-Datei zu erstellen, die mit dem Edge TPU kompatibel ist.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\textwidth]{Software/flowchart} 
	
	\caption{Der grundlegende Workflow zur Erstellung eines Modells 
		für das Edge TPU\cite{GoogleTensorFlowModel:2019}}\label{Software:Flowchart}
\end{figure}

Es ist jedoch nicht notwendig, diesen gesamten Prozess zu verfolgen, 
um ein gutes Modell für die Edge TPU zu erstellen. Stattdessen besteht 
die Möglichkeit, bestehende TensorFlow-Modelle, die mit dem Edge TPU 
kompatibel sind, zu nutzen, indem Sie sie mit Ihrem eigenen Datensatz 
umschulen. MobileNet ist beispielsweise eine beliebte 
Bildklassifizierungs-/Erkennungsmodellarchitektur, die mit dem Edge 
TPU kompatibel ist. Die Coral Beta-Community hat mehrere Versionen dieses 
Modells erstellt, die als Ausgangspunkt für die Erstellung eines eigenen 
Modells dienen können, das verschiedene Objekte erkennt. Aber wenn Sie 
Ihr eigenes Modell von Grund auf neu entworfen haben - oder planen -, 
dann lesen Sie den nächsten Abschnitt über die Modellanforderungen.

\subsection{Modellanforderungen}
Wenn Sie Ihr eigenes TensorFlow-Modell bauen möchten, das die Vorteile des Edge TPU zur Laufzeit voll ausschöpft, muss es die folgenden Anforderungen erfüllen:

\begin{itemize}
	\item Tensorparameter werden quantisiert (8-Bit-Festkommazahlen).
	Sie müssen entweder ein Quantisierungs-bewusstes Training 
	(empfohlen) oder eine vollständige ganzzahlige Quantisierung 
	nach dem Training verwenden (Sie müssen den TensorFlow 1.15
	\glqq nightly\grqq\, Build verwenden und sowohl den Input- als auch den 
	Output-Typ auf \PYTHON{uint8} setzen).
	\item Tensorgrößen sind zur Übersetzungszeit konstant (keine dynamischen Größen).
	\item Modellparameter (z.B. Bias-Tensoren) sind zur Kompilierungszeit konstant.
	\item Tensoren sind entweder 1-, 2- oder 3-dimensional. 
	Wenn ein Tensor mehr als 3 Dimensionen hat, dann können 
	nur die 3 innersten Dimensionen eine Größe größer als 1 haben.
	\item Das Modell verwendet nur die vom Edge TPU unterstützten Operationen (siehe Tabelle unten).
\end{itemize}

Wenn Ihr Modell diese Anforderungen nicht vollständig erfüllt, kann es trotzdem kompiliert werden, aber nur ein Teil des Modells wird auf der Edge TPU ausgeführt. An der ersten Stelle im Modelldiagramm, an der eine nicht unterstützte Operation stattfindet, teilt der Compiler das Diagramm in zwei Teile. Der erste Teil des Diagramms, der nur unterstützte Operationen enthält, wird zu einer benutzerdefinierten Operation kompiliert, die auf dem Edge TPU ausgeführt wird, und alles andere wird auf der CPU ausgeführt, wie in Abbildung 3.2 dargestellt.
Derzeit kann der Edge TPU-Compiler das Modell nicht mehr als einmal partitionieren, so dass, sobald eine nicht unterstützte Operation auftritt, diese Operation und alles, was danach auf der CPU ausgeführt wird, auch wenn unterstützte Operationen später auftreten.




\begin{figure}[!h]
	\centering
	\includegraphics[width=0.4\textwidth]{Software/compile-tflite} 
	
	\caption{The compiler creates a single custom op for all Edge TPU compatible ops, until it encounters an unsupported op; the rest stays the same and runs on the CPU\cite{GoogleTensorFlowModel:2019}} 
\end{figure}

Wenn man das kompilierte Modell betrachtet (mit einem Tool wie \PYTHON{visualize.py}), ist es immer noch ein TensorFlow Lite Modell, außer dass es jetzt eine benutzerdefinierte Operation am Anfang des Graphen hat. Diese benutzerdefinierte Operation ist der einzige Teil des Modells, der tatsächlich kompiliert wird - sie enthält alle Operationen, die auf dem Edge TPU ausgeführt werden. Der Rest der Grafik (beginnend mit der ersten nicht unterstützten Operation) bleibt gleich und läuft auf der CPU.

\bigskip

Wenn ein Teil des Modells auf der CPU ausgeführt wird, erwarten Sie eine deutlich verminderte Inferenzgeschwindigkeit im Vergleich zu einem Modell, das vollständig auf der Edge TPU ausgeführt wird. Die Coral-Betacommunity kann nicht vorhersagen, wie viel langsamer das Modell in dieser Situation arbeiten wird. Experimentieren Sie daher mit verschiedenen Architekturen und versuchen Sie, ein Modell zu erstellen, das zu 100 Prozent mit dem Edge TPU kompatibel ist. Das heißt, das kompilierte Modell sollte nur die benutzerdefinierte TPU-Operation Edge enthalten.

\bigskip

\begin{itemize}
	\item Wenn die Kompilierung abgeschlossen ist, sagt Ihnen der Edge TPU-Compiler, wie viele Operationen auf dem Edge TPU ausgeführt werden können und wie viele stattdessen auf der CPU ausgeführt werden müssen (falls überhaupt vorhanden). Aber beachten Sie, dass der Prozentsatz der Operationen, die auf dem Edge-TPU im Vergleich zur CPU ausgeführt werden, nicht der Gesamtperformance-Auswirkung entspricht - wenn auch nur ein kleiner Bruchteil Ihres Modells auf der CPU ausgeführt wird, kann es die Inferenz-Geschwindigkeit um eine Größenordnung verlangsamen (im Vergleich zu einer Version des Modells, die vollständig auf dem Edge-TPU läuft).
\end{itemize}

\begin{table}[htbp]
	\centering
	\begin{tabular}{|l|p{0.8\linewidth}|}
		\hline
		\multicolumn{1}{|c|}{\textit{\textbf{Operations}}} & \multicolumn{1}{c|}{\textit{\textbf{Known limitations}}}                                                   \tabularnewline \hline
		Logistic                                           &                                                   \tabularnewline \hline
		Logistic	                                      &                                                     \tabularnewline \hline
		Relu	                                              & \tabularnewline \hline
		Relu6	                                       & \tabularnewline \hline
		ReluN1To1	                                       & \tabularnewline \hline
		Tanh	                                               & \tabularnewline \hline
		Add	                                               & \tabularnewline \hline
		Maximum	                                       & \tabularnewline \hline
		Minimum	                                       & \tabularnewline \hline
		Mul	                                               & \tabularnewline \hline
		Sub	                                                & \tabularnewline \hline
		AveragePool2d	& No fused activation function.  \tabularnewline \hline
		Concatenation & 	No fused activation function.If any input is a compile-time constant tensor, there must be only 2 inputs, and this constant tensor must be all zeroes (effectively, a zero-padding op).  \tabularnewline \hline
		Conv2d	        &                           \tabularnewline \hline
		
		DepthwiseConv2d  &	Dilated conv kernels are not supported.  \tabularnewline \hline
		FullyConnected	& Only default format supported for fully-connected weights. Output tensor is one-dimensional. \tabularnewline \hline
		L2Normalization	 \tabularnewline \hline
		MaxPool2d  &	No fused activation function.  \tabularnewline \hline
		Mean & 	Supports reduction along x- and/or y-dimensions only.  \tabularnewline \hline
		Pad &	Supports padding along x- and/or y-dimensions only.  \tabularnewline \hline
		Reshape	 \tabularnewline \hline
		ResizeBilinear &	Input/output is a 3-dimensional tensor. Depending on input/output size, this operation may not be mapped to the Edge TPU to avoid loss in precision.  \tabularnewline \hline
		ResizeNearestNeighbor &	Input/output is a 3-dimensional tensor. Depending on input/output size, this operation may not be mapped to the Edge TPU to avoid loss in precision.  \tabularnewline \hline
		Slice	&       \tabularnewline \hline
		Softmax &	Supports only 1D input tensor with a max of 16,000 elements.  \tabularnewline \hline
		SpaceToDepth &	\tabularnewline \hline
		
		
	\end{tabular}
	\caption{Alle vom Edge TPU unterstützten Operationen und alle bekannten Einschränkungen.}
\end{table}


\subsection{Quantisierung}
Die Quantisierung Ihres Modells bedeutet, dass alle 32-Bit-Fließkommazahlen (wie Gewichte und Aktivierungsausgaben) in die nächsten 8-Bit-Festkommazahlen umgewandelt werden. Dies macht das Modell kleiner und schneller. Und obwohl diese 8-Bit-Darstellungen weniger präzise sein können, wird die Inferenzgenauigkeit des neuronalen Netzwerks nicht wesentlich beeinträchtigt.
Aus Kompatibilitätsgründen mit dem Edge TPU verwenden Sie entweder ein quantisierungsbewusstes (quantization-aware) Training (empfohlen) oder eine ganzzahlige
(full integer post-training) Quantisierung nach dem Training.

\section{Edge TPU-Compiler}
Der Edge TPU Compiler (edgetpu\_compiler) ist ein Kommandozeilenprogramm, das ein TensorFlow Lite Modell (.tflite Datei) in eine Datei kompiliert, die mit dem Edge TPU kompatibel ist. 

\subsection{Systemanforderungen}
Der Edge TPU-Compiler kann auf jedem modernen Debian-basierten Linux-System ausgeführt werden. Konkret die folgenden:
Debian 6.0 oder höher oder ein Derivat davon (z.B. Ubuntu 10.0+)
Systemarchitektur von x86-64 oder ARM64 mit ARMv8 Befehlssatz

\section{Edge TPU Python API Übersicht}
Die Edge TPU Python Bibliothek (das edgetpu Modul) macht es einfach, eine Inferenz mit TensorFlow Lite Modellen auf einem Edge TPU Gerät durchzuführen. Es bietet einfache APIs, die Bildklassifizierung und Objekterkennung durchführen, sowie Transfer-Learning auf dem Gerät mit Gewichtsabdruck oder Backpropagation. Sie können alle diese Funktionen ohne die Verwendung von TensorFlow-APIs nutzen - alles, was Sie benötigen, ist ein kompiliertes TensorFlow Lite Modell und die Edge TPU Python-Bibliothek.

\subsection{Installieren der Bibliothek}
Installieren Sie zunächst die neueste Edge TPU Python-Bibliothek (das Edge-PPU-Modul) auf Ihrem Host-Computer gemäß den Installationsanweisungen für Ihr Gerät.

%If you're using the Dev Board or SoM, this library is included in the Mendel system image (just be sure you've updated to the latest software).

% The USB Accelerator, you can install the library on your host computer as per the setup instructions for your device.

\subsection{Edge TPU API overview}
Key APIs in the edgetpu module that perform inferencing are the following:

\PYTHON{ClassificationEngine}: Performs image classification. Create an instance by specifying a model, and then pass an image (such as a JPEG) to \PYTHON{ClassifyWithImage()} and it returns a list of labels and scores.

\PYTHON{DetectionEngine}: Performs object detection. Create an instance by specifying a model, and then pass an image (such as a JPEG) to \PYTHON{DetectWithImage()} and it returns a list of \PYTHON{DetectionCandidate} objects, each of which contains a label, a score, and the coordinates of the object.

Both engines are subclasses of \PYTHON{BasicEngine}, which you can use to perform different types of inferencing. They both also support all image formats supported by Pillow (including JPEG, PNG, BMP), except the files must be in RGB color space (no transparency).

\subsection{\PYTHON{edgetpu.classification.engine}}

Eine Inferenzmaschine, die die Bildklassifizierung durchführt.

\medskip

\PYTHON{class edgetpu.classification.engine.ClassificationEngine(model\_path, device\_path=None)}

\medskip


Führt die Klassifizierung mit einem Bild durch.

\medskip


\textbf{Parameter:}

Erweitert \PYTHON{BasicEngine} um die Bildklassifikation mit einem bestimmten Modell.

Diese API geht davon aus, dass das angegebene Modell für die Bildklassifizierung trainiert ist.

\medskip


\textbf{Parameter:}

\PYTHON{mode\_path (str)} - Pfad zu einer TensorFlow Lite (\PYTHON{.tflite}) Datei. Dieses Modell muss für das Edge TPU kompiliert werden, ansonsten wird es einfach auf der Host-CPU ausgeführt.

\PYTHON{device\_path (str)} - Der Gerätepfad für das Edge TPU, das dieser Motor verwenden soll. Dieses Argument wird nur benötigt, wenn Sie mehrere Edge TPUs und mehr Inferenzmaschinen als verfügbare Edge TPUs haben. Weitere Informationen finden Sie in der Anleitung zur Verwendung mehrerer Edge TPUs.


\medskip


\textbf{Fehlerbehandlung:}

\PYTHON{ValueError} - Wenn die Ausgabe-Tensorgröße des Modells nicht 1 ist.


\bigskip

\PYTHON{ClassifyWithImage(img, threshold=0.1, top\_k=3, resample=0)}

Führt die Klassifizierung mit einem Bild durch.

\textbf{Parameter:}

\PYTHON{img (PIL.Image)} - Das Bild, das Sie klassifizieren möchten.

\PYTHON{Schwellenwert (float)} - Mindestvertrauensschwelle für zurückgegebene Klassifizierungen. 
Verwenden Sie beispielsweise 0,5, um nur Klassifizierungen mit einem Vertrauen gleich oder höher als 0,5 zu erhalten.

\PYTHON{top\_k (int)}  - Die maximale Anzahl der zurückzugebenden Klassifizierungen. 

\PYTHON{resample (int)} - Ein Resampling-Filter zur Größenänderung von Bildern. Dies kann eine von \PYTHON{PIL.Image.NEAREST}, \PYTHON{PIL.Image.BOX}, \PYTHON{PIL.Image.BILINEAR}, \PYTHON{PIL.Image.HAMMING}, \PYTHON{PIL.Image.BICUBIC} oder \PYTHON{PIL.Image.LANCZOS} sein. Der Standard ist \PYTHON{PIL.Image.NEAREST}. Siehe \PYTHON{Pillow}-Filter. (Hinweis: Ein komplexer Filter wie \PYTHON{PIL.Image.BICUBIC} kann eine etwas bessere Genauigkeit erzeugen, verursacht aber auch eine höhere Latenzzeit.)

\medskip


\textbf{Rückgabewert:}

Eine Liste von Klassifizierungen, von denen jede eine Liste \PYTHON{[int, float]} ist, die die Label-ID (int) und den Vertrauenswert (float) repräsentiert.

\medskip


\textbf{Fehlerbehandlung:}


\PYTHON{RuntimeError} - Wenn die Eingabe-Tensorform des Modells nicht mit der für ein Bildklassifikationsmodell erwarteten Form übereinstimmt, die \PYTHON{[1, Höhe, Breite, 3]} ist.

\PYTHON{ValueError} - Wenn Argumentwerte ungültig sind.




\section{Set up der Kamera}

%todo http://www.raspberrypi-tutorials.de/hardware/kameramodul.html

Wenn das Raspbian OS startet, klicken Sie oben links auf das Menüsymbol Raspberry Pi und wählen Sie \textsl{Einstellungen > Raspberry Pi Konfiguration}.
Klicken Sie auf Schnittstellen und stellen Sie die Kamera auf \textsl{Aktiviert} und klicken Sie auf \textsl{OK}. Es erscheint die Meldung \glqq Reboot erforderlich\grqq; klicken Sie auf \textsl{Ja}.
Zum Test der Einstellungen kann nach dem Neustart ein Terminalfenster (STRG+ALT+T)
geöffnet werden. Hier kann nun eine Probeaufnahme erstellt werden

\medskip

\PYTHON{raspistill -v -o test.jpg}

\medskip

Mit dem Befehl

\medskip

\PYTHON{xdg-open /home/pi/test.jpg}

\medskip


kann das Bild visuell geprüft werden.
Zum Test der Kamera kann auch das Programm \textsl{Piccama}
des Kameramoduls verwendet werden. Es hilft bei der Ausführung der
Echtzeit-Identifikation von Live-Objekten.
Damit das Kameramodul speziell für die Live-Objekterkennung im VNC-Viewer funktioniert, schalten Sie den Direktaufnahmemodus des VNC-Viewers ein.

%todo https://blog.helmutkarger.de/raspberry-video-camera-teil-3-raspberry-pi-kamera-modul-v2-1/


\section{Set up des USB Accelerators}

Stellen Sie sicher, dass das USB Accelerator-Gerät während der Einrichtung nicht angeschlossen ist (trennen Sie das Gerät, wenn Sie es angeschlossen haben). Öffnen Sie ein Terminal und geben Sie diese Befehle ein, um die Software herunterzuladen und zu installieren:

\medskip

\PYTHON{wget http://storage.googleapis.com/cloud-iot-edge-pretrained-models/edgetpu\_api.tar.gz}

\PYTHON{tar xzf edgetpu\_api.tar.gz}

\PYTHON{cd python-tflite-source}

\PYTHON{bash ./install.sh}



\medskip


Es ist wichtig zu beachten, dass das Skript \PYTHON{install.sh} entwickelt 
wurden, um mit Python 3.5 zu arbeiten. Damit \PYTHON{install.sh} mit 
Python 3.6, Python 3.7 Versionen funktioniert, ändern Sie die folgenden Punkte 
letzte Zeile des \PYTHON{install.sh}-Skripts (d.h. \PYTHON{setup.py}), 
anstelle von python3.5 die Versionen nach Ihren Wünschen einfügen.

\medskip

Während der Installation steht da: \glqq Möchten Sie die maximale Betriebsfrequenz aktivieren?\grqq\, Antworte vorerst mit \textsl{'n'}. Sie können es später aktivieren, wenn Sie die Leistung erhöhen möchten.

\section{Bildklassifikationsininferenz auf dem Coral USB Acclerator}

Die Datei \PYTHON{classify\_image.py} wurde direkt mit den Setup Coral USB Acclerator Dateien installiert.
Wenn Sie die folgenden Befehle ausführen, liefert der Bildklassifizierer die Ausgabe: 
\PYTHON{Score :  0.76}

\medskip


\PYTHON{cd /home/pi/python-tflite-source/edgetpu}


\PYTHON{python3 demo/ classify\_image.py}

\PYTHON{--model  $\sim$/Downloads/mobilenet\_v2\_1.0\_224\_inat\_bird\_quant\_edgetpu.tflite}

\PYTHON{--label  $\sim$/Downloads/inat\_bird\_labels.txt}

\PYTHON{--image $\sim$/Downloads/parrot.jpg}

\medskip

\subsection{Installationspozess für OpenCV\index{OpenCV!Installation} auf dem Raspberry Pi}



\href{https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/}{Quelle: Adrian Rosebrock: Raspbian Stretch: Install OpenCV 3 + Python on your Raspberry Pi \cite{Rosebrock:2017}}

\bigskip


Die Installation von OpenCV\index{OpenCV} auf dem Raspberry Pi dauer circa 3h.


\subsubsection{Step \#1: Erweiterung des Dateisystems}


Zunächst wird das Konfigurationsdialog des Raspberry Pis geöffnet:

\medskip


\PYTHON{sudo raspi-config}

\medskip

Im Konfigurationsdialog muss dann der Punkt \textsl{\glqq Advanced Options\grqq }
gewählt werden:

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\textwidth]{Software/advanced} 
	\caption{Konfigurationsdialog \glqq Advanced Options\grqq }
	
	%\href{https://www.pyimagesearch.com/wp-content/uploads/2017/08/advanced_options-768x514.png} {advancedoptions} }
\end{figure}


Dann wird der Punkt \textsl{\glqq Expand file system\grqq} gewählt:\\

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\textwidth]{Software/settings} 
	\caption{Menüpunkt \textsl{\glqq Expand file sysytem\grqq}}
\end{figure}

Nach der Aufforderung sollten Sie die erste Option 
\textsl{\glqq A1. Erweitern Sie das Dateisystem\grqq} wählen, drücken Sie die Eingabetaste auf Ihrer Tastatur, gehen Sie mit dem Pfeil nach unten zur Schaltfläche 
\textsl{\glqq <Finish>\grqq} und starten Sie Ihren Raspberry Pi neu - Sie werden vielleicht zum Neustart aufgefordert, aber wenn nicht, können Sie ihn ausführen:

\medskip

\PYTHON{sudo reboot}

\medskip

Nach dem Neustart sollte Ihr Dateisystem um den gesamten verfügbaren Speicherplatz auf Ihrer Micro-SD-Karte erweitert worden sein. Sie können überprüfen, ob die Festplatte erweitert wurde, indem Sie \PYTHON{df -h} ausführen.

\subsubsection{Step \#2: Installation der Abhängigkeiten}

\PYTHON{sudo apt-get update \&\& sudo apt-get upgrade}

\medskip

Wir müssen dann einige Entwicklerwerkzeuge installieren, darunter CMake, das uns bei der Konfiguration des OpenCV-Build-Prozesses unterstützt:

\medskip

\PYTHON{sudo apt-get install build-essential cmake pkg-config}

\medskip


Als nächstes müssen wir einige Image-I/O-Pakete installieren, die es uns ermöglichen, verschiedene Image-Dateiformate von der Festplatte zu laden. Beispiele für solche Dateiformate sind JPEG, PNG, TIFF, etc.:

\medskip

\PYTHON{sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev}

\medskip

Just as we need image I/O packages, we also need video I/O packages. These libraries allow us to read various video file formats from disk as well as work directly with video streams:

\medskip

\PYTHON{sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev}

\PYTHON{sudo apt-get install libxvidcore-dev libx264-dev}

\medskip

The OpenCV library comes with a sub-module named highgui which is used to display images to our screen and build basic GUIs. In order to compile the highgui module, we need to install the GTK development library:

\medskip

\PYTHON{sudo apt-get install libgtk2.0-dev libgtk-3-dev}

\PYTHON{sudo apt-get install libgtk2.0-dev libgtk-3-dev}

\medskip

Many operations inside of OpenCV (namely matrix operations) can be optimized further by installing a few extra dependencies:\

\medskip

\PYTHON{sudo apt-get install libatlas-base-dev gfortran}

\medskip

Lastly, let's install both the Python 2.7 and Python 3 header files so we can compile OpenCV with Python bindings:

\medskip

\PYTHON{sudo apt-get install python2.7-dev python3-dev}

\medskip

If you skip this step, you may notice an error related to the \PYTHON{Python.h} header file not being found when running make to compile OpenCV.

Now that we have our dependencies installed, let's grab the 3.3.0 archive of OpenCV from the official OpenCV repository.


\medskip

\PYTHON{cd $\sim$ }

\PYTHON{wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.3.0.zip}

\PYTHON{unzip opencv.zip}

\medskip

We'll want the full install of OpenCV 3 (to have access to features such as SIFT and SURF, for instance), so we also need to grab the \PYTHON{opencv\_contrib} repository as well:


\medskip

\PYTHON{wget -O opencv\_contrib.zip}

\PYTHON{https://github.com/Itseez/opencv\_contrib/archive/3.3.0.zip}

\PYTHON{unzip opencv\_contrib.zip}

\medskip

\textbf{Step \#4: Python 2.7 or Python 3}

Before we can start compiling OpenCV on our Raspberry Pi 3, we first need to install \PYTHON{pip}, a Python package manager:

\medskip

\PYTHON{wget https://bootstrap.pypa.io/get-pip.py}

\PYTHON{sudo python get-pip.py}

\medskip

\textbf{Installing NumPy on your Raspberry Pi}

\medskip

\PYTHON{pip install numpy}

\medskip

\textbf{Step \#5: Compile and Install OpenCV}


we can setup our build using CMake:


\medskip

\PYTHON{cd $\sim$/opencv-3.3.0/}

\PYTHON{mkdir build}

\PYTHON{cd build}

\PYTHON{cmake -D CMAKE\_BUILD\_TYPE=RELEASE \ }

\PYTHON{      -D CMAKE\_INSTALL\_PREFIX=/usr/local \ }

\PYTHON{      -D INSTALL\_PYTHON\_EXAMPLES=ON \ }

\PYTHON{      -D OPENCV\_EXTRA\_MODULES\_PATH=$\sim$/opencv\_contrib-3.3.0/modules \ }

\PYTHON{      -D BUILD\_EXAMPLES=ON ..}

\medskip


\textbf{Configure your swap space size before compiling.}


Before you start the compile process, you should increase your swap space size. This enables OpenCV to compile with all four cores of the Raspberry PI without the compile hanging due to memory problems.

Open your \PYTHON{/etc/dphys-swapfile}  and then edit the 
\PYTHON{CONF\_SWAPSIZE}  variable:

\begin{verbatim}
# set size to absolute value, leaving empty (default) then uses computed value
#you most likely don't want this, unless you have an special disk situation
CONF_SWAPSIZE=100
CONF_SWAPSIZE=1024
\end{verbatim}

Notice that I've commented out the 100MB line and added a 1024MB line. This is the secret to getting compiling with multiple cores on the Raspbian Stretch.

If you skip this step, OpenCV might not compile.

To activate the new swap space, restart the swap service:

\begin{verbatim}
sudo /etc/init.d/dphys-swapfile stop
sudo /etc/init.d/dphys-swapfile start
\end{verbatim}
It is possible to burn out the Raspberry Pi microSD card because flash memory has a limited number of writes until the card won’t work. It is highly recommended that you change this setting back to the default when you are done compiling and testing the install (see below). To read more about swap sizes corrupting memory, see this page.

\begin{verbatim}
make -j4
\end{verbatim}

From there, all need to do is install OpenCV 3 on your Raspberry Pi 3:

\begin{verbatim}
sudo make install
sudo ldconfig
\end{verbatim}

\textbf{Step \#6: Finish installing OpenCV on your Pi}

After running \PYTHON{make install}, your OpenCV + Python bindings should be installed in \PYTHON{/usr/local/lib/python3.5/dist-packages}. Again, you can verify this with the ls command.

When compiling OpenCV 3 bindings for Python 3+, the output .so file is named cv2.cpython-35m-arm-linux-gnueabihf.so (or some variant of) rather than simply cv2.so (like in the Python 2.7 bindings).

To fix this bug we need to rename the file:

\begin{verbatim}
cd /usr/local/lib/python3.5/dist-packages/
sudo mv cv2.cpython-35m-arm-linux-gnueabihf.so cv2.so
\end{verbatim}

\textbf{Step \#7: Testing your OpenCV 3 install}

\begin{verbatim}
python3
>>> import cv2
>>> cv2.__version__
'3.3.0'
\end{verbatim}

In the classify\_images.py python file input image is preprocessed with Open CV library and classifier provides the name of the bird categorie  on  top of the input image.
The classify\_images file is present in the raspberry pi.The program in the file is inspired from the code written by the Adrian of \href{https://www.pyimagesearch.com/2019/05/13/object-detection-and-image-classification-with-google-coral-usb-accelerator/}{pyimagesearch.com}.The path of classify\_image.py file in the raspberry pi \begin{verbatim}
/home/pi/python-tflite-source/edgetpu/demo/classify_images.py
\end{verbatim}

\medskip

The file name: \PYTHON{classify\_images.py}

\medskip


\begin{verbatim}
1.# import the necessary packages
2.from edgetpu.classification.engine import ClassificationEngine
3.from PIL import Image
4.import argparse
5.import imutils
6.import time
7.import cv2

9.# construct the argument parser and parse the arguments
10.ap = argparse.ArgumentParser()
11.ap.add_argument("-m", "--model", required=True,
12.	help="path to TensorFlow Lite classification model")
13.ap.add_argument("-l", "--labels", required=True,
14.	help="path to labels file")
15.ap.add_argument("-i", "--image", required=True,
16.	help="path to input image")
17.args = vars(ap.parse_args())

\end{verbatim}
We start of by importing packages. Most notably, we are importing ClassificationEngine  from edgetpu  on Line 2.\newline

From there we’ll parse three command line arguments via Lines 10-17:\newline
--model : The path to our TensorFlow Lite classifier.\newline
--labels : Class labels file path associated with our model.\newline
--image : Our input image path.\newline

Let’s go ahead and load the labels :\newline
%\begin{lstlisting}[breaklines]
\begin{verbatim}
19.# initialize the labels dictionary
20.print("[INFO] parsing class labels...")
21.labels = {}
23.# loop over the class labels file
24.for row in open(args["labels"]):
25.	# unpack the row and update the labels dictionary
26.	(classID, label) = row.strip().split(" ", maxsplit=1)
27.	labels[int(classID)] = label.strip()

\end{verbatim}

Lines 21-27 facilitate loading class labels  from a text file into a Python dictionary. Later on, the Coral API will return the predicted classID  (an integer). We can then take that integer class label and lookup the associated  label  value in this dictionary.\newline

Moving on, now let’s load our classification model  with the edgetpu  API:\newline
%\begin{lstlisting}[breaklines]
\begin{verbatim}
29.# load the Google Coral classification model
30.print("[INFO] loading Coral model...")
31.model = ClassificationEngine(args["model"])
\end{verbatim}

Our pre-trained TensorFlow Lite classification model  is instantiated via the ClassificationEngine  class (Line 31) where we pass in the path to our model via command line argument.\newline
Let’s go ahead and load + preprocess our image :\newline
%\begin{lstlisting}[breaklines]
\begin{verbatim}
33.# load the input image
34.image = cv2.imread(args["image"])
35.image = imutils.resize(image, width=500)
36.orig = image.copy()
37.
38.# prepare the image for classification by converting (1) it from BGR
39.# to RGB channel ordering and then (2) from a NumPy array to PIL
40.# image format
41.image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
42.image = Image.fromarray(image)

\end{verbatim}
Our image  is loaded (Line 34) and then preprocessed (Lines 35-42).\newline

Take note that we made an original copy  of the image — we’ll be annotating this copy of the image with the output predictions later in the script.\newline
%\begin{lstlisting}[breaklines]
\begin{verbatim}
45.# make predictions on the input image
46.print("[INFO] making predictions...")
47.start = time.time()
48.results = model.ClassifyWithImage(image, top_k=3)
49.end = time.time()
50.print("[INFO] classification took {:.4f} seconds...".format(
end - start))

\end{verbatim}
On Line 47, we make classification predictions on the input image using the ClassifyWithImage  function (a super easy one-liner function call). The edgetpu  API allows us to specify that we only want the top results with the top\_k  parameter.\newline
Timestamps are sandwiched around this classification line and the elapse time is then printed via Lines 49 and 50.\newline

From here we’ll process the results :\newline
%\begin{lstlisting}[breaklines]
\begin{verbatim}
52.# loop over the results
53.for (i, (classID, score)) in enumerate(results):
54.	# check to see if this is the top result, and if so, draw the
55.	# label on the image
56.	if i == 0:
57.		text ="Label: {}, {:.2f}".format(labels[classID],score )
58.			
59.		cv2.putText(orig, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,0.8, (0, 0, 255), 2)

60.			
61.# display the classification result to the terminal
62.	print("{}. {}: {:.2f}".format(i + 1, labels[classID],
63.		score ))
64.
65. 
66.# show the output image
67.cv2.imshow("Image", orig)
68.cv2.waitKey(0)

\end{verbatim}
Looping over the results  (Line 53) we first find the top result and annotate the image with the label and  score (Lines 56-60).

For good measure, we’ll also print the other results and scores (but only in our terminal) via Lines 63 and 64.

Finally, the annotated original (OpenCV format) image is displayed to the screen (Lines 67 and 68).

%\begin{lstlisting}[breaklines]
\begin{verbatim}
cd /Downloads
wget https://dl.google.com/coral/canned_models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite
wget https://dl.google.com/coral/canned_models/inat_bird_labels.txt
wget https://coral.withgoogle.com/static/docs/images/parrot.jpg

\end{verbatim}
Now navigate to the directory where we've shared the sample scripts and run image classification with the parrot image.
%\begin{lstlisting}[breaklines]
\begin{verbatim}
cd  /home/pi/python-tflite-source/edgetpu
python3 demo/classify_images.py 
--model $\sim$ /Downloads/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite
--labels  $\sim$/Downloads/inat_bird_labels.txt 
--image $\sim$ /Downloads/parrot.jpg
\end{verbatim}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.2\textwidth]{Software/parrot}
	\caption{Parrot.jpg: Googles Beispiel zur Klassifikation von Bildern
		%\href{https://coral.withgoogle.com/examples/classify-image/}{parrot}
		\cite{GoogleCoral:2019}}
	
\end{figure}
\clearpage 
After running set of commands metioned above, the scripts outputs class as well as the score in the picture:

Ara macao (Scarlet Macaw)

Score :  0.74
%Inspired n from the  program image classification on coral Usb accleator coded by Adrian, a similar procedure was followed to program this  image classification program .\newline


